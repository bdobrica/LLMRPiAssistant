# RPI Assistant Configuration
# This file takes precedence over environment variables.
# Remove or comment out any settings you want to control via environment variables.

[audio]
# Audio device configuration
sample_rate = 16000
# 80ms @ 16kHz
chunk_size = 1280
# Number of input channels
channels = 4
# Which channel to use (0-3)
mic_channel_index = 0
# Substring to match audio device name
device_match = seeed

[wakeword]
# Wake word detection settings
# Detection threshold (0.0-1.0, higher = less sensitive)
threshold = 0.5
# Minimum time between wake word detections
cooldown_seconds = 1.0

# Comma-separated list of wake word models to load
# Leave empty or comment out to load all default models
# Example models: hey_jarvis, alexa, hey_mycroft, hey_rhasspy
# Use fewer models to reduce memory usage on limited devices
# models = hey_jarvis
models =

[recording]
# Voice recording settings
# Maximum recording length
max_duration_seconds = 10.0
# Stop recording after this much silence
silence_hold_seconds = 0.8
# RMS level below which is considered silence
# Lower = more sensitive, stops on quieter sounds
# Higher = less sensitive, needs louder silence
# Typical values: 0.005 (very sensitive) to 0.02 (less sensitive)
silence_rms_threshold = 0.007
# How much audio to capture before wake word
pre_roll_seconds = 0.4
# Where to save recorded audio
output_path = /tmp/command.wav

[openai]
# OpenAI API configuration
# REQUIRED: Set your API key here or via OPENAI_API_KEY environment variable
api_key = 

# Whisper transcription settings
# Whisper model to use
whisper_model = whisper-1

# Chat completion settings
# Chat model (gpt-4o-mini, gpt-4o, gpt-3.5-turbo, etc.)
chat_model = gpt-4o-mini
# Maximum tokens in response
max_tokens = 500
# Response creativity (0.0-2.0)
temperature = 0.7

# System prompt for the assistant
system_prompt = You are a helpful voice assistant running on a Raspberry Pi. Provide concise, friendly responses suitable for voice interaction.

# Text-to-Speech (TTS) settings
tts_voice = alloy
tts_model = tts-1

# Embedding settings for episodic memory
embedding_model = text-embedding-3-small

[led]
# LED configuration
# Number of LEDs (12 for ReSpeaker 4-Mic Array, 3 for 2-Mic pHAT)
count = 12

[audio_output]
# Audio output configuration for TTS playback
# Enable/disable audio output
enabled = true
# ALSA device for playback (e.g., hw:0,0, default, plughw:0,0)
device = hw:0,0
# Where to save TTS response audio file
tts_output_path = /tmp/response.mp3

[logging]
# Logging configuration
# Where to save interaction logs
log_file = openai_interactions.log
# Logging level (DEBUG, INFO, WARNING, ERROR)
log_level = INFO

[memory]
# Episodic memory configuration
# Enable/disable memory retrieval during conversations
retrieval_enabled = true
# Session idle timeout in seconds (commits episode to memory after this period of silence)
idle_timeout = 20.0
# Embedding vector dimension (must match embedding_model)
# text-embedding-3-small = 1536, text-embedding-3-large = 3072
embedding_dim = 1536
# Directory to store memory data (vectors, summaries, manifest)
memory_path = assistant_memory
# Initial capacity for vector storage (grows automatically as needed)
initial_capacity = 1024
# Number of top memories to retrieve for context
retrieval_top_k = 5
